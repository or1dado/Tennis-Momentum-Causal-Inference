{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iUXjgZdJq1-o"
   },
   "outputs": [],
   "source": [
    "# !pip install catboost\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "suKkviCUjVd1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from utils import plot_calibration_curve, plot_overlap\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r46Rfa8ujiTp"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wsDTRIZ8jd4t"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Final_Data/feature_df.csv', index_col=0)\n",
    "# train_df = pd.read_csv('Final_Data/train_feature_df.csv', index_col=0)\n",
    "# eval_df = pd.read_csv('Final_Data/eval_feature_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_class = partial(LogisticRegression, max_iter=10000)\n",
    "svm_model_class = partial(SVC, probability=True)\n",
    "rf_model_class = partial(RandomForestClassifier)\n",
    "gb_model_class = partial(GradientBoostingClassifier)\n",
    "xgb_model_class = partial(xgb.XGBClassifier)\n",
    "\n",
    "# with open('Models.pkl', 'rb') as f:\n",
    "#     models = pickle.load(f)\n",
    "\n",
    "# with open('SLearner_Models.pkl', 'rb') as f:\n",
    "#     slearner_hp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "uw6h8v0wq9HN",
    "outputId": "a03ab308-955c-4332-99e3-50194cf11e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "def estemite_e(df, model):\n",
    "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    \n",
    "    X = df.drop(columns=['T', 'Y'])\n",
    "    y = df['T']\n",
    "    \n",
    "    if type(model).__name__ in [\"LogisticRegression\", \"SVC\", \"RandomForestClassifier\",\"GradientBoostingClassifier\", \"XGBClassifier\"]:\n",
    "        dummied_df = pd.get_dummies(X[categorical_cols], prefix=categorical_cols).astype(int)\n",
    "        X = pd.concat([X.drop(categorical_cols, axis=1), dummied_df], axis=1)\n",
    "    \n",
    "    if type(model).__name__ == ['LogisticRegression', 'SVC']:\n",
    "        scaler = MinMaxScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    print(type(model).__name__)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    return y_pred_proba\n",
    "\n",
    "############################ Propensity eval ################################\n",
    "propensity_score_lr = estemite_e(df, lr_model_class())\n",
    "propensity_score_svm = estemite_e(df, svm_model_class())\n",
    "propensity_score_rf = estemite_e(df, rf_model_class())\n",
    "propensity_score_gb = estemite_e(df, gb_model_class())\n",
    "propensity_score_xgb = estemite_e(df, xgb_model_class())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(df['T'], {'Logistic Regression': propensity_score_lr,\n",
    "                                        'SVM': propensity_score_svm,\n",
    "                                        'Random Forest': propensity_score_rf,\n",
    "                                        'GRADIENT BOOSTING': propensity_score_gb,\n",
    "                                        'XGBOOST': propensity_score_xgb,\n",
    "                                        })\n",
    "\n",
    "plot_overlap(df['T'], propensity_score_lr, model_name='Logistic Regression')\n",
    "plot_overlap(df['T'], propensity_score_svm, model_name='SVM')\n",
    "plot_overlap(df['T'], propensity_score_rf, model_name='Random Forest')\n",
    "plot_overlap(df['T'], propensity_score_gb, model_name='GRADIENT BOOSTING')\n",
    "plot_overlap(df['T'], propensity_score_xgb, model_name='XGBOOST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulo_7I6ekHUJ"
   },
   "source": [
    "# IPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdP3LyAvtmzH",
    "outputId": "39975b29-f1ad-4c67-87b9-5452ec99aac0"
   },
   "outputs": [],
   "source": [
    "def IPW_ATE(df, propensity_score):\n",
    "    n = len(df)\n",
    "    y1 = df.loc[df['T'] == 1, 'Y']\n",
    "    y0 = df.loc[df['T'] == 0, 'Y']\n",
    "\n",
    "    assert n == len(y1) + len(y0), \"Mismatch\"\n",
    "\n",
    "    ATT_ipw = (sum(y1 / propensity_score[df['T'] == 1]) - sum(y0 / (1 - propensity_score[df['T'] == 0]))) / n\n",
    "\n",
    "    return ATT_ipw\n",
    "\n",
    "#############################################################################\n",
    "IPW_ATE_df = IPW_ATE(df, propensity_score_lr)\n",
    "\n",
    "ATE_df = pd.DataFrame({'Method': ['IPW'], 'ATE': [IPW_ATE_df]})\n",
    "ATE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Model For S/T-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, model, test_size=0.2, random_state=42):\n",
    "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    X = df.drop(columns=['Y'])\n",
    "    y = df['Y']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    if type(model).__name__ in [\"LogisticRegression\", \"SVC\", \"RandomForestClassifier\", \"GradientBoostingClassifier\", \"XGBClassifier\"]:\n",
    "        X_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "        X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "        \n",
    "        X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "    if type(model).__name__ in ['LogisticRegression', 'SVC']:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "        X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        'Train F1 Score': f1_score(y_train, y_train_pred),\n",
    "        'Test F1 Score': f1_score(y_test, y_test_pred),\n",
    "        'Train Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'Test Accuracy': accuracy_score(y_test, y_test_pred),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "#############################################################################\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": lr_model_class(),\n",
    "    \"Support Vector Machine\": svm_model_class(),\n",
    "    \"Random Forest\": rf_model_class(),\n",
    "    \"Gradient Boosting\": gb_model_class(),\n",
    "    \"XGBoost\": xgb_model_class()\n",
    "}\n",
    "\n",
    "all_metrics = {}\n",
    "for model_name, model in models.items():\n",
    "    metrics = evaluate_model(df, model)\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "results_df = pd.DataFrame(all_metrics).T\n",
    "results_df = results_df.reset_index().melt(id_vars='index', var_name='Metric', value_name='Score')\n",
    "results_df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "results_df.set_index(['Metric', 'Model'], inplace=True)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Metric', y='Score', hue='Model', data=results_df.reset_index(), errorbar=None)\n",
    "plt.title('Model Performance by Metric')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca3GjzBey9Bx"
   },
   "source": [
    "# S-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYUomzJ_p6uR",
    "outputId": "fe8daaaa-cc2f-4556-c346-9f903ce29859"
   },
   "outputs": [],
   "source": [
    "def SLearner_ATE(df, model):\n",
    "  categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "\n",
    "  # X_test = df_test.drop(columns=['Y'])\n",
    "  # y_test = df_test['Y']\n",
    "\n",
    "  X = df.drop(columns=['Y'])  \n",
    "  y = df['Y']\n",
    "\n",
    "  if type(model).__name__ in [\"LogisticRegression\", \"RandomForestClassifier\", \"GradientBoostingClassifier\", \"XGBClassifier\"]:\n",
    "    dummied_df = pd.get_dummies(X[categorical_cols], prefix=categorical_cols).astype(int)\n",
    "    X = pd.concat([X.drop(categorical_cols, axis=1), dummied_df], axis=1)\n",
    "\n",
    "  if type(model).__name__ in ['LogisticRegression']:\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "  if type(model).__name__ in ['CatBoostClassifier']:\n",
    "      model.fit(X, y, cat_features=categorical_cols)\n",
    "  else:\n",
    "      model.fit(X, y)\n",
    "\n",
    "  x1 = X.copy()\n",
    "  x0 = X.copy()\n",
    "  x1['T'], x0['T'] = 1, 0\n",
    "\n",
    "  y1_pred = model.predict(x1)\n",
    "  y0_pred = model.predict(x0)\n",
    "\n",
    "  ATE = (y1_pred - y0_pred).mean()\n",
    "  return ATE\n",
    "\n",
    "#############################################################################\n",
    "SLearner_ATE_df = SLearner_ATE(df, model=rf_model_class())\n",
    "new_row = pd.DataFrame({'Method': ['S-learner'], 'ATE': [SLearner_ATE_df]})\n",
    "ATE_df = pd.concat([ATE_df, new_row], ignore_index=True)\n",
    "ATE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgK1m0eQ74GK"
   },
   "source": [
    "# T-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ga1vbI465xeQ",
    "outputId": "37dab4d4-7c2d-4ff5-d6ed-2a2992481075"
   },
   "outputs": [],
   "source": [
    "def TLearner_ATE(df, model0, model1):\n",
    "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "\n",
    "    X = df.drop(columns=['Y']).reset_index(drop=True)\n",
    "    y = df['Y'].reset_index(drop=True)\n",
    "    treatment = df['T'].reset_index(drop=True)\n",
    "\n",
    "    if type(model1).__name__ in ['LogisticRegression', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']:\n",
    "        dummied_df_X = pd.get_dummies(X[categorical_cols], prefix=categorical_cols).astype(int)\n",
    "        X = pd.concat([X.drop(categorical_cols, axis=1), dummied_df_X], axis=1)\n",
    "\n",
    "    if type(model1).__name__ == 'LogisticRegression':\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    X1 = X[treatment == 1].drop(columns=['T'])\n",
    "    y1 = y[treatment == 1].reset_index(drop=True)\n",
    "    X0 = X[treatment == 0].drop(columns=['T'])\n",
    "    y0 = y[treatment == 0].reset_index(drop=True)\n",
    "\n",
    "    if type(model1).__name__ == 'CatBoostClassifier':\n",
    "        model1.fit(X1, y1, cat_features=categorical_cols)\n",
    "        model0.fit(X0, y0, cat_features=categorical_cols)\n",
    "    else:\n",
    "        model1.fit(X1, y1)\n",
    "        model0.fit(X0, y0)\n",
    "\n",
    "    y1_pred = model1.predict(X.drop(columns=['T']))\n",
    "    y0_pred = model0.predict(X.drop(columns=['T']))\n",
    "\n",
    "    ATE = (y1_pred - y0_pred).mean()\n",
    "    return ATE\n",
    "\n",
    "#############################################################################\n",
    "SLearner_ATE_df = TLearner_ATE(df, model0=lr_model_class(), model1=lr_model_class())\n",
    "print(\"Estimated ATE (Logistic Regression):\", SLearner_ATE_df)\n",
    "\n",
    "SLearner_ATE_df = TLearner_ATE(df, model0=rf_model_class(), model1=rf_model_class())\n",
    "print(\"Estimated ATE (Random Forest):\", SLearner_ATE_df)\n",
    "\n",
    "SLearner_ATE_df = TLearner_ATE(eval_df, model0=gb_model_class(), model1=gb_model_class())\n",
    "print(\"Estimated ATE (Gradient Boosting):\", SLearner_ATE_df)\n",
    "\n",
    "SLearner_ATE_df = TLearner_ATE(eval_df, model0=xgb_model_class(), model1=xgb_model_class())\n",
    "print(\"Estimated ATE (XGBoost):\", SLearner_ATE_df)\n",
    "\n",
    "\n",
    "TLearner_ATE_df = TLearner_ATE(eval_df, model0=rf_model_class(), model1=rf_model_class())\n",
    "\n",
    "new_row = pd.DataFrame({'Method': ['T-learner'], 'ATE': [TLearner_ATE_df]})\n",
    "ATE_df = pd.concat([ATE_df, new_row], ignore_index=True)\n",
    "ATE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g-WKRIjAd8h"
   },
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjQHT5CP9jcn",
    "outputId": "fd153c8a-628c-44c8-ce71-5a1891dceb58"
   },
   "outputs": [],
   "source": [
    "def Matching_ATE(df, epsilon=0.1):\n",
    "    df_norm = df.copy()\n",
    "    df_norm = df_norm.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    categorical_cols = df_norm.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = df_norm.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    numerical_cols = [col for col in numerical_cols if col not in ['Y', 'T']]\n",
    "\n",
    "    dummied_df_X = pd.get_dummies(df_norm[categorical_cols], drop_first=True)\n",
    "    X = pd.concat([df_norm[numerical_cols], dummied_df_X, df_norm[['T', 'Y']]], axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "    X1 = X[X['T'] == 1].reset_index(drop=True)\n",
    "    X0 = X[X['T'] == 0].reset_index(drop=True)\n",
    "\n",
    "    # Use NearestNeighbors to find closest matches\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='euclidean').fit(X0.drop(columns=['T', 'Y']).values)\n",
    "    distances, min_indices = nbrs.kneighbors(X1.drop(columns=['T', 'Y']).values)\n",
    "\n",
    "    # Filter for valid matches\n",
    "    valid_matches = distances.flatten() < epsilon\n",
    "    valid_min_indices = min_indices.flatten()[valid_matches]\n",
    "    counts = np.bincount(valid_min_indices)\n",
    "    unique_indices = np.where(counts == 1)[0]\n",
    "\n",
    "    # Keep only treated indices that correspond to unique matches\n",
    "    treated_indices_with_unique_matches = np.where(valid_matches)[0][np.isin(valid_min_indices, unique_indices)]\n",
    "\n",
    "    if len(treated_indices_with_unique_matches) > 0:\n",
    "        treated_Y_values = X1['Y'].values[treated_indices_with_unique_matches]\n",
    "        control_Y_values = X0.loc[valid_min_indices[np.isin(valid_min_indices, unique_indices)], 'Y'].values\n",
    "\n",
    "        # Calculate ATE\n",
    "        ATE = treated_Y_values - control_Y_values\n",
    "        return ATE.mean()\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "#############################################################################\n",
    "epsilon = 0.1  \n",
    "matching_ATE = Matching_ATE(eval_df, epsilon)\n",
    "\n",
    "new_row = pd.DataFrame({'Method': ['Matching'], 'ATE': [matching_ATE]})\n",
    "ATE_df = pd.concat([ATE_df, new_row], ignore_index=True)\n",
    "ATE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gkf7qsp4LPjy"
   },
   "source": [
    "# Doubly - Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R77e-ZkILUZs",
    "outputId": "7dbb1f4f-b16d-4cd3-d9be-22626dbd046a"
   },
   "outputs": [],
   "source": [
    "def DR_ATE(df, model0, model1, propensity_score):\n",
    "    model_name = type(model1).__name__\n",
    "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "\n",
    "    X = df.drop(columns=['Y'])\n",
    "    y = df['Y']\n",
    "\n",
    "    if model_name in ['LogisticRegression', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier']:\n",
    "      dummied_df_X = pd.get_dummies(X[categorical_cols], prefix=categorical_cols).astype(int)\n",
    "      X = pd.concat([X.drop(categorical_cols, axis=1), dummied_df_X], axis=1)\n",
    "\n",
    "    if model_name in ['LogisticRegression']:\n",
    "      scaler = MinMaxScaler()\n",
    "      X_scaled = scaler.fit_transform(X)\n",
    "      X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    X1, y1 = X[X['T'] == 1].drop(columns=['T']), y[X['T'] == 1]\n",
    "    X0, y0 = X[X['T'] == 0].drop(columns=['T']), y[X['T'] == 0]\n",
    "\n",
    "    if model_name in ['CatBoostClassifier']:\n",
    "        model1.fit(X1, y1, cat_features=categorical_cols)\n",
    "        model0.fit(X0, y0, cat_features=categorical_cols)\n",
    "    else:\n",
    "       model1.fit(X1, y1)\n",
    "       model0.fit(X0, y0)\n",
    "\n",
    "    y1_pred = model1.predict(X.drop(columns=['T']))\n",
    "    y0_pred = model0.predict(X.drop(columns=['T']))\n",
    "\n",
    "    # ATE\n",
    "    g1 = y1_pred + (df['T'].to_numpy() / propensity_score)*(y - y1_pred)\n",
    "    g0 = y0_pred + ((1-df['T'].to_numpy()) / (1-propensity_score))*(y - y0_pred)\n",
    "    return (g1 - g0).mean()\n",
    "\n",
    "#############################################################################\n",
    "DR_ATE_df = DR_ATE(eval_df, model0=rf_model_class(), model1=rf_model_class(), propensity_score=propensity_score_lr)\n",
    "\n",
    "new_row = pd.DataFrame({'Method': ['Doubly Robust'], 'ATE': [DR_ATE_df]})\n",
    "ATE_df = pd.concat([ATE_df, new_row], ignore_index=True)\n",
    "ATE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bootstap_CI(df, method, propensity_score, model_class):\n",
    "    df['propensity_score'] = propensity_score\n",
    "    print(method)\n",
    "\n",
    "    ATEs = []\n",
    "    for _ in range(300):\n",
    "         sample_df = df.sample(n=len(df), replace=True) \n",
    "         propensity_score = sample_df['propensity_score'].to_numpy()\n",
    "         sample_df = sample_df.drop(columns='propensity_score')\n",
    "\n",
    "         if method == \"IPW\":\n",
    "            result = IPW_ATE(sample_df, propensity_score)\n",
    "         elif method == \"S-learner\":\n",
    "           result = SLearner_ATE(sample_df, model_class())\n",
    "         elif method == \"T-learner\":\n",
    "           result = TLearner_ATE(sample_df, model_class(), model_class())\n",
    "         elif method == \"Matching\":\n",
    "           result = Matching_ATE(sample_df, epsilon=0.1)\n",
    "         elif method == \"Doubly Robust\":\n",
    "           result = DR_ATE(sample_df, model_class(), model_class(), propensity_score)\n",
    "         else:\n",
    "           raise ValueError(\"Unknown method:\", method)\n",
    "         ATEs.append(result)\n",
    "\n",
    "    lower_bound = round(np.percentile(ATEs, 2.5), 3)\n",
    "    upper_bound = round(np.percentile(ATEs, 97.5), 3)\n",
    "    return [lower_bound, upper_bound]\n",
    "\n",
    "#############################################################################\n",
    "ATE_df['CI'] = ATE_df['Method'].apply(lambda method: calc_bootstap_CI(eval_df, method, propensity_score_lr, rf_model_class))\n",
    "ATE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hfOuO2nkhw2m",
    "outputId": "9c166979-4337-4c69-8d45-2184baab0201"
   },
   "outputs": [],
   "source": [
    "display(ATE_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
